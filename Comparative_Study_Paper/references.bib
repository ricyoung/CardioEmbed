@article{lora2021,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{loraplus2024,
  title={LoRA+: Efficient Low Rank Adaptation of Large Models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@article{lorareview2024,
  title={Low-Rank Adaptation for Foundation Models: A Comprehensive Review},
  author={Li, Zhen and Zhang, Yifan and Wang, Shuai and Chen, Jie and Liu, Yang},
  journal={arXiv preprint arXiv:2501.00365},
  year={2024}
}

@inproceedings{infonce,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  booktitle={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{biolinkbert,
  title={LinkBERT: Pretraining Language Models with Document Links},
  author={Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8003--8016},
  year={2022}
}

@inproceedings{mpnet,
  title={MPNet: Masked and Permuted Pre-training for Language Understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16857--16867},
  year={2020}
}

@inproceedings{sentencebert2019,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={3982--3992},
  year={2019}
}

@inproceedings{simcse2021,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={6894--6910},
  year={2021}
}

@inproceedings{scibert2019,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={3615--3620},
  year={2019}
}

@article{clinicalbert2019,
  title={Publicly Available Clinical BERT Embeddings},
  author={Alsentzer, Emily and Murphy, John R and Boag, Willie and Weng, Wei-Hung and Jin, Di and Naumann, Tristan and McDermott, Matthew},
  journal={arXiv preprint arXiv:1904.03323},
  year={2019}
}

@article{biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020}
}

@article{bge,
  title={C-Pack: Packaged Resources To Advance General Chinese Embedding},
  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas},
  journal={arXiv preprint arXiv:2309.07597},
  year={2023}
}

@article{e5,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@article{jina,
  title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents},
  author={G{\"u}nther, Michael and Ong, Jackmin and Mohr, Isabelle and Abdessalem, Alaeddine and Abel, Tanguy and Akram, Mohammad Kalim and Guzman, Susana and Mastrapas, Georgios and Sturua, Saba and Wang, Bo and Werk, Maximilian and Wang, Nan and Xiao, Han},
  journal={arXiv preprint arXiv:2310.19923},
  year={2023}
}

@article{gemma,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={{Gemma Team}},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{qwen,
  title={Qwen Technical Report},
  author={{Qwen Team}},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{huggingface,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/2020.emnlp-demos.6},
  url={https://aclanthology.org/2020.emnlp-demos.6/}
}

@misc{bitsandbytes,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2110.02861},
  year={2021}
}

@article{multiple_negatives_ranking,
  title={Efficient Natural Language Response Suggestion for Smart Reply},
  author={Henderson, Matthew and Al-Rfou, Rami and Strope, Brian and Sung, Yun-Hsuan and Luk{\'a}cs, L{\'a}szl{\'o} and Guo, Ruiqi and Kumar, Sanjiv and Miklos, Balint and Kurzweil, Ray},
  journal={arXiv preprint arXiv:1705.00652},
  year={2017}
}

@book{cohen1988,
  title={Statistical power analysis for the behavioral sciences},
  author={Cohen, Jacob},
  year={1988},
  publisher={Lawrence Erlbaum Associates}
}

@article{holm1979,
  title={A simple sequentially rejective multiple test procedure},
  author={Holm, Sture},
  journal={Scandinavian Journal of Statistics},
  volume={6},
  number={2},
  pages={65--70},
  year={1979},
  publisher={JSTOR}
}

@article{cardioembed2024,
  title={CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology},
  author={Young, Richard J and Matthews, Alice M},
  journal={arXiv preprint arXiv:2511.10930},
  year={2024}
}

@article{ablation2019,
  title={Ablation Studies in Artificial Neural Networks},
  author={Meyes, Richard and Lu, Melanie and de Puiseau, Constantin Waubert and Meisen, Tobias},
  journal={arXiv preprint arXiv:1901.08644},
  year={2019}
}

@article{hyperparameter_ablation2023,
  title={Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study},
  author={Kargaran, Amir Hossein and Yvon, Fran{\c{c}}ois and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2308.06017},
  year={2023}
}

@article{sensitivity_analysis2022,
  title={Goal-Oriented Sensitivity Analysis of Hyperparameters in Deep Learning},
  author={Wickstr{\o}m, Kristoffer and Kampffmeyer, Michael and Jenssen, Robert},
  journal={arXiv preprint arXiv:2207.06216},
  year={2022}
}
