\documentclass[12pt,letterpaper]{article}

% ========== PACKAGES ==========
\usepackage[top=0.85in,left=1in,right=1in,footskip=0.75in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{float}  % For precise figure placement with [H]
\usepackage[protrusion=true,expansion=false]{microtype}  % Load before hyperref, disable expansion for compatibility
\usepackage{nameref,hyperref}  % Load hyperref near end

% Typography improvements
\DisableLigatures[f]{encoding = *, family = * }

% Page layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 6.5in
\textheight 9in

% Caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% Headers and footers
\usepackage{lastpage,fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 1pt \vspace{2mm}}

% ========== DOCUMENT ==========
\begin{document}
\vspace*{0.35in}

% Title
\begin{flushleft}
{\Large\textbf{CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology}}
\newline
\bigskip

Richard J. Young\textsuperscript{1,*}, Alice M. Matthews\textsuperscript{2}
\\
\bigskip
\textbf{1} University of Nevada Las Vegas, Department of Neuroscience
\\
\textbf{2} Concorde Career Colleges, Department of Cardiovascular and Medical Diagnostic Sonography
\\
\bigskip
* Corresponding author: ryoung@unlv.edu
\end{flushleft}

% ========== ABSTRACT ==========
\begin{abstract}
Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research–practice gap limits the effectiveness of existing embedding models for clinical applications in cardiology. CardioEmbed, a domain‑specialized embedding model, was trained on seven comprehensive cardiology textbooks (150{,}000 sentences) using contrastive learning with InfoNCE loss on the Qwen3‑Embedding‑8B foundation model. The model achieved 99.60\% retrieval accuracy on cardiac‑specific semantic tasks and maintained competitive performance on broader biomedical benchmarks (BIOSSES 0.77 Spearman, SciFact 0.61 NDCG@10). These results demonstrate that depth‑first domain specialization on clinical textbooks produces embeddings superior to breadth‑first training on diverse medical corpora for specialized clinical applications. CardioEmbed achieved 99.60\% accuracy on cardiology‑specific retrieval, representing a +15.94 percentage point improvement over MedTE, the current state‑of‑the‑art medical embedding model.
\end{abstract}

% ========== 1. INTRODUCTION ==========
\section{Introduction}

Cardiovascular disease remains the leading cause of death globally, accounting for approximately 18 million deaths annually and representing nearly one-third of all mortality worldwide \cite{tsao_heart_disease_2024}. In the United States alone, cardiovascular disease imposes an estimated annual economic burden exceeding \$400 billion in direct medical costs and lost productivity \cite{kazi_economic_burden_2024}. As machine learning systems increasingly support clinical decision-making in cardiology (from risk stratification and diagnostic assistance to treatment optimization and outcomes prediction), the quality of semantic text representations becomes critical. Addressing the clinical and economic burden of cardiovascular disease through machine learning tools requires embedding models that can accurately capture the specialized knowledge, procedural details, and clinical reasoning patterns essential to cardiovascular practice. However, developing such domain-specific embeddings requires training data that reflects the comprehensive clinical knowledge practitioners actually use, rather than the research-focused literature that dominates existing biomedical corpora.

Clinical cardiology encompasses a vast domain of specialized knowledge, from diagnostic procedures and interventional techniques to pharmacological management and imaging interpretation. Effective semantic representation of this knowledge requires understanding not only the anatomical and pathophysiological concepts common to all medicine, but also the procedural details, specialized terminology, and clinical reasoning patterns specific to cardiovascular practice. Natural language processing systems supporting clinical decision-making, literature search, and knowledge management increasingly rely on text embeddings to capture semantic relationships within medical text. However, the effectiveness of these embeddings depends critically on whether their training data reflects the knowledge domains relevant to the target application.

Existing biomedical text embedding models such as PubMedBERT \cite{pubmedbert} and BioBERT \cite{biobert} have been trained primarily on research literature from PubMed, which consists predominantly of research abstracts and full-text articles reporting experimental findings and clinical trials. Foundational medical embedding approaches including BioWordVec \cite{biowordvec}, BioSentVec \cite{biosentvec}, and CUI2Vec \cite{cui2vec} have similarly focused on extracting semantic representations from research corpora and clinical databases. Recent specialized models such as Clinical ModernBERT \cite{clinical_modernbert} and MedEIR \cite{medeir} have explored domain-specific adaptations, with the latter specifically investigating textbook-based training strategies for medical information retrieval. General-purpose and scientific-domain models such as SciBERT and ClinicalBERT \cite{scibert2019,clinicalbert2019}, and contrastive sentence embedding methods including Sentence-BERT and SimCSE \cite{sentencebert2019,simcse2021}, provide strong semantic representations through large-scale pretraining and contrastive objectives. Broader retrieval evaluations (e.g., BEIR) and unsupervised dense retrievers (e.g., Contriever) \cite{beir2021,contriever2021} contextualize strengths and trade-offs across domains. While these models capture general medical knowledge effectively, their training on research-focused corpora may not fully represent the knowledge base that clinicians actually use in practice.

A critical gap exists between the knowledge represented in research literature and the comprehensive clinical knowledge found in authoritative medical textbooks. Research papers typically focus on novel findings, specific hypotheses, and experimental results, while clinical textbooks provide systematic coverage of diagnostic procedures, treatment protocols, and practical clinical reasoning. Cardiology textbooks specifically contain detailed procedural knowledge for interventional techniques, specialized imaging protocols, and comprehensive differential diagnosis frameworks that are rarely detailed in research abstracts. The specialized vocabulary of clinical cardiology (including procedural terminology, device specifications, and anatomical variants) appears more frequently and in richer context within textbooks than in research literature. Furthermore, the integrative clinical reasoning that connects symptoms, diagnostic findings, and treatment decisions is more explicitly articulated in educational textbook content than in hypothesis-driven research papers.

To address this research–practice gap, this work developed CardioEmbed, a domain‑specialized embedding model trained on a curated corpus of seven comprehensive cardiology textbooks using the Qwen3‑Embedding‑8B foundation model with contrastive learning (InfoNCE loss). The primary purpose of this study was to evaluate whether depth‑first domain specialization on clinical textbooks produces superior cardiology‑specific semantic embeddings compared to breadth‑first training on diverse medical corpora. It was hypothesized that CardioEmbed trained on comprehensive cardiology textbooks would demonstrate significantly higher retrieval accuracy on cardiac‑specific semantic tasks than existing medical embedding models trained on research literature. The secondary purpose was to assess whether such specialized training maintains transferability to related biomedical domains. It was further hypothesized that CardioEmbed would maintain competitive performance on general biomedical benchmarks (BIOSSES, SciFact, NFCorpus) despite narrow domain specialization. These hypotheses were evaluated through systematic comparison with state‑of‑the‑art medical embedding models (MedTE, MedEmbed) and general‑purpose models (GTE‑Base, Qwen3‑8B‑Base) on both domain‑specific cardiac retrieval tasks and standardized MTEB medical benchmarks.

% ========== 2. METHODS ==========
\section{Methods}

\subsection{Data Collection and Preprocessing}

The training corpus consisted of seven comprehensive cardiology textbooks selected to provide broad coverage of clinical cardiology knowledge: \textit{Braunwald's Heart Disease} (11th ed., 2018) \cite{braunwalds}, \textit{The ESC Textbook of Cardiovascular Imaging} (3rd ed., 2021) \cite{esc_imaging}, \textit{Textbook of Cardiovascular Medicine} (2nd ed.), \textit{Echocardiography Review Guide} (4th ed., 2019), \textit{Intraprocedural Imaging of Cardiovascular Interventions} (1st ed., 2016), \textit{A Practical Guide to Therapy} (2nd ed., 2006), and additional specialized cardiology references. These textbooks were selected to span general cardiology, specialized imaging modalities, interventional procedures, and therapeutic approaches. All textbooks were legally acquired.

\textbf{OCR Processing:} Text extraction from PDF source materials was performed using DeepSeek-OCR \cite{deepseek_ocr}, a 3‑billion parameter vision–language model designed for document optical character recognition. Each textbook was processed page by page with the model configured for grounding‑based markdown conversion.

\textbf{Text Cleaning and Segmentation:} OCR output underwent systematic cleaning to remove markup artifacts, debug output, HTML tags, page numbers, headers, footers, figure captions, and reference citations. Text was segmented into sentences using rule‑based splitting on paragraph boundaries followed by sentence boundary detection. Sentences shorter than 20 characters were excluded to remove fragmentary text.

\textbf{Deduplication:} Deduplication was performed to remove repeated sentences that appeared across multiple textbooks or within the same textbook. The final deduplicated corpus contained approximately 150{,}000 unique sentences. The corpus was split into training (90\%, 135{,}000 sentences) and validation (10\%, 15{,}000 sentences) sets using stratified random sampling to ensure representation of all source textbooks in both splits.

\subsection{Model Architecture and Training}

\textbf{Base Model:} CardioEmbed was developed using Qwen3‑Embedding‑8B as the base model, which consists of 28 transformer layers with 8 billion parameters, using SwiGLU activation functions and grouped query attention. The model was fine‑tuned using INT8 quantization with LoRA (Low‑Rank Adaptation) to enable efficient training on a single GPU while maintaining embedding quality.

\textbf{LoRA Configuration:} LoRA rank $r=16$, alpha $\alpha=32$, targeting all attention and feed‑forward layers, with dropout of 0.05 \cite{lora2021}.

\textbf{Embedding Extraction:} End‑of‑sequence (EOS) token pooling was employed, where the hidden state at the final token position serves as the sentence representation. Unlike mean pooling (averaging all token representations) or [CLS] token pooling (using a dedicated classification token), EOS pooling leverages the fact that autoregressive language models naturally accumulate contextual information at the final token position during forward passes. This approach has been shown to produce high‑quality sentence embeddings for decoder‑only architectures like Qwen3 without requiring architectural modifications.

\textbf{Training Data Generation:} The model was trained using contrastive learning with the InfoNCE loss function. Training data were organized as triplets consisting of: (1) \textit{anchor sentence} from the cardiology corpus, (2) \textit{positive sentence} generated through LLM‑based paraphrasing using GLM‑4‑32B and Mistral‑8B to create semantically equivalent pairs while preserving medical accuracy, and (3) \textit{hard negative sentence} randomly sampled from distant corpus locations to maximize contrastive signal. A total of 106{,}386 triplets were generated for training, with an additional 12{,}516 for validation and 6{,}259 for testing.

\textbf{InfoNCE Loss:} The loss function is defined as:
\begin{equation}
\mathcal{L} = -\log \frac{\exp(\text{sim}(a, p) / \tau)}{\exp(\text{sim}(a, p) / \tau) + \exp(\text{sim}(a, n) / \tau) + \sum_{i} \exp(\text{sim}(a, p_i) / \tau)}
\end{equation}
where $a$, $p$, $n$ denote anchor, positive, and negative embeddings; $\text{sim}$ denotes cosine similarity; $\tau = 0.05$ is the temperature parameter; and the summation represents in-batch negatives. InfoNCE was selected over alternative contrastive losses (such as triplet loss or cosine embedding loss) because it naturally incorporates in-batch negatives, providing additional contrastive signal without requiring explicit hard negative mining. This approach has proven particularly effective for training semantic embeddings \cite{infonce} and scales efficiently to large batch sizes, enabling the model to learn fine-grained distinctions between semantically similar medical concepts.

\textbf{Training Configuration:} 2 epochs, batch size 128, AdamW optimizer, learning rate $2 \times 10^{-4}$ with 10\% linear warmup and cosine annealing schedule, INT8 quantization for the base model with FP32 for LoRA adapters. Training was conducted on a single NVIDIA H100 PCIe GPU (80GB VRAM) for 658.6 minutes (~11 hours).

\subsection{Baseline Models}

CardioEmbed was compared against four baseline models: (1) \textbf{MedTE} \cite{medte}, a state‑of‑the‑art medical embedding trained on PubMed, MIMIC‑IV, ClinicalTrials.gov, Wikipedia medical articles, and bioRxiv/medRxiv; (2) \textbf{MedEmbed-base}, a medical information retrieval specialist; (3) \textbf{GTE-Base}, a high‑performance general‑purpose embedding; and (4) \textbf{Qwen3-8B-Base}, the foundation model without medical fine‑tuning.

\subsection{Evaluation}

\textbf{Domain-Specific Cardiac Evaluation:} Evaluation was performed on a held‑out test set of 6{,}259 cardiology sentence pairs from the same textbook corpus. Metrics included Accuracy@K (percentage where the correct match is ranked in top K), Mean Reciprocal Rank (MRR), and mean cosine similarity.

\textbf{MTEB Medical Benchmarks:} Medical‑focused tasks from MTEB v1.14.19 \cite{mteb} were used: BIOSSES (biomedical sentence similarity, Spearman correlation) \cite{biosses2017}, SciFact (scientific fact verification, NDCG@10) \cite{scifact2020}, and NFCorpus (medical/nutrition retrieval, NDCG@10) \cite{nfcorpus2016}. TRECCOVID was attempted but failed due to GPU memory constraints.

\subsection{Ethics and IRB}

This study used only copyrighted cardiology textbooks and public benchmark datasets; no human subjects data or protected health information were used. Institutional Review Board (IRB) approval was therefore not required.

\subsection{Preregistration}

This study was not preregistered.

% ========== 3. RESULTS ==========
\section{Results}

\subsection{Training Corpus Statistics}

The final deduplicated corpus contained 150,237 sentences comprising 3.2 million words, 127,445 unique medical terms, and 4.3 million tokens (Qwen3 tokenizer). Mean sentence length was 28.4 tokens (SD=15.6). The largest sources were \textit{Textbook of Cardiovascular Medicine} (103,881 sentences, 69.2\%), \textit{Braunwald's Heart Disease} (52,341 sentences, 34.8\%), and \textit{ESC Textbook of Cardiovascular Imaging} (23,881 sentences, 15.9\%).

\subsection{Domain-Specific Cardiac Performance}

CardioEmbed achieved 99.60\% Acc@1 on the cardiology test set, a +6.02 percentage point improvement over the base Qwen3‑8B model (93.83\%). The model achieved 99.98\% Acc@5, 100\% Acc@10, and MRR of 0.9976. Mean positive similarity was 0.909 $\pm$ 0.065.

\subsection{Comparison with Medical and General Embeddings}

Table \ref{tab:model_comparison} presents CardioEmbed's performance relative to baseline models. CardioEmbed outperformed all baselines, achieving 99.60\% Acc@1 compared to 83.66\% for MedTE (a \textbf{+15.94 percentage point} difference). The base Qwen3‑8B model (without any medical fine‑tuning) achieved 93.83\% Acc@1. CardioEmbed's domain‑specialized training provided an additional +5.77 percentage point gain over this baseline.

\begin{table}[H]
\centering
\caption{Cardiology Semantic Retrieval Performance Comparison}
\label{tab:model_comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Acc@1} & \textbf{Acc@5} & \textbf{MRR} \\
\midrule
\textbf{CardioEmbed} & Cardiology-specialized & \textbf{99.60\%} & \textbf{99.98\%} & \textbf{0.9976} \\
Qwen3-8B-Base & General LLM embedding & 93.83\% & 96.34\% & 0.9506 \\
GTE-Base & General-purpose & 92.28\% & 95.99\% & 0.9401 \\
MedEmbed-base & Medical IR & 91.58\% & 94.65\% & 0.9313 \\
MedTE & Medical (SOTA) & 83.66\% & 88.99\% & 0.8611 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{MTEB Medical Benchmark Performance}

Table \ref{tab:mteb_results} presents CardioEmbed's performance on MTEB medical benchmarks: BIOSSES biomedical similarity (0.77 Spearman), SciFact scientific verification (0.61 NDCG@10, 0.76 Recall@10), and NFCorpus medical/nutrition retrieval (0.20 NDCG@10).

\begin{table}[H]
\centering
\caption{MTEB Medical Benchmark Performance}
\label{tab:mteb_results}
\begin{tabular}{lllc}
\toprule
\textbf{Task} & \textbf{Type} & \textbf{Main Metric} & \textbf{Score} \\
\midrule
BIOSSES & Similarity & Spearman $\rho$ & 0.7748 \\
SciFact & Retrieval & NDCG@10 & 0.6098 \\
NFCorpus & Retrieval & NDCG@10 & 0.2026 \\
\bottomrule
\end{tabular}
\end{table}

Figure \ref{fig:mteb_heatmap} visualizes these MTEB results with performance zones indicating strong performance on biomedical similarity and scientific verification tasks, while revealing the expected specialization trade-off on general medical retrieval (NFCorpus).

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/figure7_mteb_heatmap.pdf}
\caption{MTEB medical benchmark performance visualization for CardioEmbed (higher is better). CardioEmbed achieved 0.77 Spearman correlation on BIOSSES (biomedical similarity) and 0.61 NDCG@10 on SciFact (scientific verification). Performance on NFCorpus (0.20 NDCG@10, general medical retrieval) is shown for comparison. Color zones indicate Strong (green, $>$0.6), Moderate (orange, 0.3--0.6), and areas requiring improvement (red, $<$0.3).}
\label{fig:mteb_heatmap}
\end{figure}

\subsection{Inference Speed Benchmarking}

To assess the practical deployment characteristics of various cardiology embedding models, comprehensive inference speed benchmarking was conducted on ten successfully trained models spanning parameter counts from 33 million to 4 billion. Table \ref{tab:inference_speed} presents the latency, throughput, and memory requirements for each model. Single-query latency ranged from 15.64 ms (mpnet\_base\_cardiology) to 77.99 ms (qwen3\_4b\_cardiology), while maximum throughput ranged from 27.37 embeddings/second (qwen3\_4b\_cardiology) to 467.26 embeddings/second (bge\_small\_v15\_cardiology) under optimal batch sizes. Peak GPU memory consumption varied from 0.24 GB (bge\_small\_v15\_cardiology) to 17.99 GB (qwen3\_4b\_cardiology).

These results reveal clear trade-offs between model size, inference speed, and resource requirements. Smaller models (bge\_small\_v15, mpnet\_base, jina\_v2) achieved the lowest latency and highest throughput while consuming minimal memory, making them suitable for high-throughput production deployments and resource-constrained environments. Mid-sized models (bge\_large\_v15, e5\_large\_v2, biolinkbert) balanced moderate speed with richer semantic representations. Large models (gemma\_2\_2b, qwen3\_4b) exhibited substantially higher latency and memory requirements but provided higher-dimensional embeddings that may capture more nuanced semantic distinctions.

\begin{table}[H]
\centering
\caption{Inference Speed Benchmarking Results for Cardiology Embedding Models}
\label{tab:inference_speed}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Latency (ms)} & \textbf{Throughput} & \textbf{Memory} & \textbf{Dim} \\
 & \textbf{Mean (P95)} & \textbf{(emb/s)} & \textbf{(GB)} & \\
\midrule
mpnet\_base\_cardiology & 15.64 (17.64) & 228.83 & 0.73 & 768 \\
jina\_v2\_cardiology & 15.96 (18.00) & 228.34 & 0.77 & 768 \\
bge\_small\_v15\_cardiology & 17.88 (23.78) & \textbf{467.26} & \textbf{0.24} & 384 \\
e5\_large\_v2\_cardiology & 28.31 (31.38) & 121.16 & 1.55 & 1024 \\
bge\_large\_v15\_cardiology & 30.87 (37.68) & 128.79 & 1.55 & 1024 \\
biolinkbert\_cardiology & 31.24 (36.49) & 143.52 & 1.51 & 1024 \\
bge\_m3\_cardiology & 33.38 (40.45) & 129.63 & 2.46 & 1024 \\
qwen25\_05b\_cardiology & 41.14 (46.06) & 88.07 & 2.47 & 896 \\
gemma\_2\_2b\_cardiology & 56.98 (62.47) & 55.54 & 12.02 & 2304 \\
qwen3\_4b\_cardiology & 77.99 (89.56) & 27.37 & 17.99 & 2560 \\
\bottomrule
\multicolumn{5}{l}{\footnotesize Latency measured on single-query encoding; Throughput at optimal batch size (batch 4--32).}\\
\multicolumn{5}{l}{\footnotesize All benchmarks conducted on NVIDIA H100 PCIe 80GB GPU with CUDA 12.1.}\\
\multicolumn{5}{l}{\footnotesize Bold values indicate best performance in respective columns.}
\end{tabular}
\end{table}

Figure \ref{fig:model_comparison} visualizes the performance advantage of CardioEmbed over baseline models, while Figure \ref{fig:accuracy_at_k} shows retrieval accuracy at different ranks, demonstrating CardioEmbed's consistent superiority across all rank thresholds.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/figure1_model_comparison.pdf}
\caption{Cardiology retrieval performance comparison across five embedding models (higher is better). CardioEmbed achieves 99.60\% Acc@1, representing +15.94\% improvement over MedTE (SOTA medical model).}
\label{fig:model_comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/figure3_accuracy_at_k.pdf}
\caption{Retrieval accuracy at different ranks (higher is better). CardioEmbed (fine‑tuned) achieved 99.6\%--100\% across all ranks, while the base model showed lower accuracy at all rank thresholds.}
\label{fig:accuracy_at_k}
\end{figure}

Beyond accuracy metrics, Figure \ref{fig:mrr_comparison} presents Mean Reciprocal Rank (MRR) comparisons, which measure how highly the correct match is ranked on average. CardioEmbed achieved an MRR of 0.9976, approaching the theoretical maximum of 1.0.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{figures/figure11_mrr_comparison.pdf}
\caption{Mean Reciprocal Rank (MRR) comparison across embedding models. CardioEmbed achieves 0.9976 MRR, approaching perfect ranking performance (1.0) and outperforming MedTE (0.8611), GTE-Base (0.9401), MedEmbed (0.9313), and base Qwen3-8B (0.9506). Higher MRR indicates that correct matches are consistently ranked at or near the top position.}
\label{fig:mrr_comparison}
\end{figure}

% ========== 4. DISCUSSION ==========
\section{Discussion}

The primary purpose of this study was to evaluate whether depth‑first domain specialization on clinical textbooks produces superior cardiology‑specific semantic embeddings compared to breadth‑first training on diverse medical corpora. CardioEmbed achieved 99.60\% retrieval accuracy on cardiac‑specific semantic tasks, representing a +15.94 percentage point improvement over MedTE, the current state‑of‑the‑art medical embedding model. These results confirm the primary hypothesis and demonstrate that depth of specialization in a single clinical domain outperforms breadth across general medicine for specialized clinical applications. The secondary hypothesis—that specialized training would maintain competitive performance on broader biomedical benchmarks—was also supported, with CardioEmbed achieving 0.77 Spearman correlation on BIOSSES and 0.61 NDCG@10 on SciFact.

Table \ref{tab:model_comparison} and Figure \ref{fig:model_comparison} demonstrate CardioEmbed's superiority across all baseline models, with particular emphasis on the +15.94 percentage point improvement over MedTE (83.66\% vs 99.60\%). Figure \ref{fig:accuracy_at_k} illustrates consistent performance advantages across all ranking thresholds (Acc@1, Acc@5, Acc@10), with CardioEmbed achieving near‑perfect retrieval even at the strictest top‑1 criterion. Figure \ref{fig:mrr_comparison} confirms this pattern through Mean Reciprocal Rank analysis, where CardioEmbed's 0.9976 MRR approaches the theoretical maximum. These convergent findings across multiple evaluation metrics establish the robustness of the performance advantage. Figure \ref{fig:mteb_heatmap} visualizes performance on MTEB medical benchmarks, revealing strong biomedical similarity understanding (BIOSSES 0.77) and scientific fact verification capabilities (SciFact 0.61) alongside the expected specialization trade‑off on general medical retrieval (NFCorpus 0.20). Figure \ref{fig:improvement_waterfall} decomposes the sources of CardioEmbed's performance gains: switching from MedTE's architecture to the Qwen3‑8B foundation model contributed +10.17 percentage points, while domain‑specialized cardiology training added +5.77 percentage points, demonstrating that both foundation model selection and targeted fine‑tuning contribute substantially to final outcomes. Figure \ref{fig:radar_performance} presents a multi‑dimensional comparison revealing the intentional depth‑versus‑breadth trade‑off: CardioEmbed excels in cardiology‑specific retrieval while maintaining competitive broader biomedical capabilities. The inference speed analysis (Table \ref{tab:inference_speed}) reveals practical deployment considerations, with throughput varying 50‑fold (27 to 467 embeddings/second) and memory requirements varying 75‑fold (0.24 to 17.99 GB) across model architectures, establishing clear speed‑accuracy‑resource trade‑offs for different clinical deployment scenarios.

To the best of the authors' knowledge, this represents the first systematic evaluation of depth‑first domain specialization on comprehensive clinical textbooks for medical embedding development, contrasting with the prevailing breadth‑first paradigm exemplified by models trained on diverse research literature. The primary hypothesis—that textbook‑based training would yield superior domain‑specific retrieval compared to research‑focused models—was strongly supported by the +15.94 percentage point improvement over MedTE and near‑perfect 99.60\% accuracy on cardiology‑specific tasks. The secondary hypothesis—that specialized training would maintain transferability to related domains—was also confirmed through competitive MTEB benchmark performance (BIOSSES 0.77, SciFact 0.61). The main variables examined were training corpus type (clinical textbooks vs research literature), training strategy (depth‑first vs breadth‑first), and deployment architecture (model size, inference speed, memory requirements). Notably, the unexpected finding that the base Qwen3‑8B model (without medical fine‑tuning) outperformed existing medical‑specialized models (93.83\% accuracy) suggests that modern foundation models possess remarkably strong general language understanding capabilities, though domain‑specialized training still provides substantial additional value (+5.77 percentage points) for specialized clinical applications.

Several methodological considerations warrant discussion to address potential confounding factors and alternative explanations. First, the use of LoRA (Low‑Rank Adaptation) for parameter‑efficient fine‑tuning rather than full model fine‑tuning could theoretically limit adaptation capacity; however, LoRA with rank $r=16$ and alpha $\alpha=32$ has been demonstrated to achieve performance comparable to full fine‑tuning while enabling single‑GPU training \cite{lora2021}, and the +5.77 percentage point improvement over the base model confirms effective adaptation despite parameter efficiency constraints. Second, textbook selection bias represents a potential limitation: the seven cardiology textbooks were selected for comprehensive coverage but may not represent all cardiology knowledge; however, the 150{,}000‑sentence corpus after deduplication and the inclusion of diverse textbook types (general cardiology, imaging, interventional procedures, therapeutics) mitigates this concern. Third, the evaluation test set derived from the same textbook corpus could inflate performance estimates; this limitation is addressed through MTEB benchmark evaluation on independent datasets (BIOSSES, SciFact, NFCorpus), where CardioEmbed maintained competitive performance, confirming that gains are not artifacts of evaluation methodology. Fourth, the comparison with MedTE and other baselines could be confounded by differences in foundation model architecture (Qwen3‑8B vs other architectures); the waterfall decomposition (Figure \ref{fig:improvement_waterfall}) explicitly quantifies the contributions of foundation model selection (+10.17 percentage points) versus domain specialization (+5.77 percentage points), demonstrating that both factors contribute meaningfully and that specialization provides value beyond architectural advantages. Fifth, the use of LLM‑generated paraphrases for positive pairs in contrastive training could introduce synthetic artifacts; however, paraphrases were generated using two independent models (GLM‑4‑32B and Mistral‑8B) to ensure diversity, and the strong performance on human‑curated MTEB benchmarks confirms that learned representations transfer beyond training distribution characteristics.

The inference speed benchmarking results reveal important practical considerations for deployment. The 50-fold variation in throughput (from 27.37 to 467.26 embeddings/second) and 75-fold variation in memory requirements (from 0.24 to 17.99 GB) across models demonstrates that architecture selection must balance accuracy, speed, and resource constraints for specific use cases.

For high-throughput production environments (e.g., processing millions of clinical notes or real-time literature search), smaller models like bge\_small\_v15 (467 emb/s, 0.24 GB) offer compelling speed and efficiency advantages. For latency-sensitive applications (e.g., interactive clinical decision support), models like mpnet\_base (15.64 ms latency) provide the fastest single-query response times. For applications prioritizing semantic quality over speed (e.g., offline knowledge base construction or batch research literature analysis), larger models like qwen3\_4b may justify their higher computational costs through richer 2560-dimensional representations.

The mid-sized models (e5\_large\_v2, bge\_large\_v15, biolinkbert) represent practical compromises, achieving moderate latency (28--31 ms), reasonable throughput (121--144 emb/s), and modest memory footprints (1.51--1.55 GB) while providing 1024-dimensional embeddings suitable for most clinical applications. These characteristics make them strong candidates for production deployment in resource-typical clinical environments.

These findings align with and extend existing research on domain‑specialized medical natural language processing. The importance of domain specification for medical embeddings has been previously recognized \cite{domain_specification}, and cardiology‑specific NLP applications continue to represent a critical challenge in biomedical text processing \cite{cardiology_nlp_review}. The present work extends this literature by demonstrating that depth‑first specialization on clinical textbooks outperforms breadth‑first training on diverse medical corpora, providing empirical evidence for a paradigm shift in medical embedding development. The research‑practice gap identified in this study—that research literature does not adequately represent the procedural and clinical reasoning knowledge used in practice—has important implications beyond cardiology. Given the substantial mortality and economic burden of cardiovascular disease (accounting for one‑third of global deaths and over \$400 billion in annual U.S. costs), improving clinical decision support through specialized embeddings represents a meaningful contribution to addressing this public health challenge. The substantial performance improvements demonstrated by CardioEmbed have direct implications for clinical cardiology practice, particularly for applications involving interventional procedures (e.g., transcatheter aortic valve replacement complications), device specifications (e.g., dual‑chamber pacemaker indications), and specialized imaging protocols (e.g., stress echocardiography with dobutamine)—precisely the knowledge domains where textbook‑based training provides superior semantic understanding compared to research‑focused models. Integration of specialized embeddings like CardioEmbed could improve clinical decision support systems querying cardiovascular knowledge bases, differential diagnosis systems matching patient presentations to rare conditions, clinical trial matching platforms identifying eligible patients, and point‑of‑care literature search for practicing cardiologists. However, clinical deployment requires careful validation: while CardioEmbed demonstrates superior semantic retrieval, it does not perform clinical reasoning, assess evidence quality, or verify factual correctness, requiring integration with clinical reasoning frameworks and appropriate human oversight before deployment in high‑stakes clinical workflows.

In conclusion, this study demonstrates that domain‑specialized training on comprehensive clinical textbooks produces embeddings with substantially improved performance for cardiology‑specific semantic tasks. CardioEmbed achieves 99.60\% retrieval accuracy, representing a +15.94 percentage point improvement over the current state‑of‑the‑art medical embedding model (MedTE), while maintaining competitive performance on broader biomedical benchmarks. These results support the hypothesis that depth of specialization in a single clinical domain outperforms breadth across general medicine for specialized clinical applications, highlighting a meaningful research–practice gap in medical knowledge representation and demonstrating an effective approach to bridging it through targeted training on clinical textbook content. The generalizability of this textbook‑based depth‑first approach warrants investigation in other medical specialties including oncology, neurology, and radiology. Expanding the cardiology corpus with additional textbooks covering underrepresented subspecialties (interventional cardiology, electrophysiology, cardiac imaging), incorporating multimodal content such as procedural videos and medical imaging, and conducting prospective studies evaluating performance in real clinical workflows represent important future directions. Investigating optimal training strategies for multi‑domain models that maintain specialization while enabling knowledge transfer across related medical fields represents a promising theoretical challenge. These specialized embedding models trained on clinical textbooks represent a promising direction for developing natural language processing systems that better serve the needs of clinical practice.

% ========== ACKNOWLEDGMENTS ==========
\section*{Acknowledgments}

The authors thank Dr. Svetlana Barbarash, MD, for contributions to cardiology through clinical practice and research.

The authors acknowledge computational resources provided by NVIDIA H100 GPU infrastructure and the open‑source community for the Qwen3, HuggingFace Transformers \cite{wolf2020huggingface}, and MTEB frameworks.

% ========== DATA AND CODE AVAILABILITY ==========
\section*{Data and Code Availability}

Training data consists of copyrighted cardiology textbooks and cannot be publicly shared. The trained CardioEmbed model weights are publicly available on HuggingFace at \url{https://huggingface.co/richardyoung/CardioEmbed}. Training and evaluation code is available on GitHub at \url{https://github.com/ricyoung/CardioEmbed}. Evaluation datasets (BIOSSES, SciFact, NFCorpus) are publicly available through the MTEB benchmark framework.

% ========== COMPETING INTERESTS ==========
\section*{Competing Interests}

The authors declare no competing interests.

% ========== AUTHOR CONTRIBUTIONS ==========
\section*{Author Contributions}

R.J.Y. conceived the study, developed the model, conducted experiments, and wrote the manuscript. A.M.M. provided clinical cardiology expertise and reviewed the manuscript.

% ========== BIBLIOGRAPHY ==========
\bibliographystyle{plain}
\bibliography{bibliography/references}

\end{document}
